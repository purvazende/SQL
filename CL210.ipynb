{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7izWfaHRtfUx"
      },
      "outputs": [],
      "source": [
        "def pagerank(G, alpha=0.85, personalization=None, max_iter=100, tol=1.0e-6, nstart=None, weight='weight', dangling=None):\n",
        "    if len(G) == 0:\n",
        "        return {}\n",
        "\n",
        "    if not G.is_directed():\n",
        "        D = G.to_directed()\n",
        "    else:\n",
        "        D = G\n",
        "\n",
        "    # Create a copy in (right) stochastic form\n",
        "    W = nx.stochastic_graph(D, weight=weight)\n",
        "    N = W.number_of_nodes()\n",
        "\n",
        "    # Choose fixed starting vector if not given\n",
        "    if nstart is None:\n",
        "        x = dict.fromkeys(W, 1.0 / N)\n",
        "    else:\n",
        "        # Normalized nstart vector\n",
        "        s = float(sum(nstart.values()))\n",
        "        x = dict((k, v / s) for k, v in nstart.items())\n",
        "\n",
        "    if personalization is None:\n",
        "        # Assign uniform personalization vector if not given\n",
        "        p = dict.fromkeys(W, 1.0 / N)\n",
        "    else:\n",
        "        missing = set(G) - set(personalization)\n",
        "        if missing:\n",
        "            raise NetworkXError('Personalization dictionary must have a value for every node. Missing nodes %s' % missing)\n",
        "        s = float(sum(personalization.values()))\n",
        "        p = dict((k, v / s) for k, v in personalization.items())\n",
        "\n",
        "    if dangling is None:\n",
        "        # Use personalization vector if dangling vector not specified\n",
        "        dangling_weights = p\n",
        "    else:\n",
        "        missing = set(G) - set(dangling)\n",
        "        if missing:\n",
        "            raise NetworkXError('Dangling node dictionary must have a value for every node. Missing nodes %s' % missing)\n",
        "        s = float(sum(dangling.values()))\n",
        "        dangling_weights = dict((k, v/s) for k, v in dangling.items())\n",
        "\n",
        "    dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]\n",
        "\n",
        "     # power iteration: make up to max_iter iterations\n",
        "    for _ in range(max_iter):\n",
        "        xlast = x\n",
        "        x = dict.fromkeys(xlast.keys(), 0)\n",
        "        danglesum = alpha * sum(xlast[n] for n in dangling_nodes)\n",
        "        for n in x:\n",
        "            # this matrix multiply looks odd because it is\n",
        "            # doing a left multiply x^T=xlast^T*W\n",
        "            for nbr in W[n]:\n",
        "                x[nbr] += alpha * xlast[n] * W[n][nbr][weight]\n",
        "            x[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n]\n",
        "\n",
        "        # check convergence, l1 norm\n",
        "        err = sum([abs(x[n] - xlast[n]) for n in x])\n",
        "        if err < N*tol:\n",
        "            return x\n",
        "    raise NetworkXError('Pagerank: power iteration failed to converge in %d iterations.' % max_iter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx"
      ],
      "metadata": {
        "id": "4IpjHLg4tstL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.barabasi_albert_graph(60, 41)\n",
        "pr = nx.pagerank(G, 0.4)"
      ],
      "metadata": {
        "id": "dc1k1Q5Htoyd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnUjtCjztuhi",
        "outputId": "11c54bc9-336b-4be6-f3c9-e3ec0c510819"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.028060461857347126, 1: 0.012760328222114365, 2: 0.012571576490056354, 3: 0.012366587669864355, 4: 0.012965582893731753, 5: 0.012951284690860026, 6: 0.012764378213312161, 7: 0.012957826818110446, 8: 0.012748583123464888, 9: 0.012991961940040165, 10: 0.013773607979725315, 11: 0.012553519325363431, 12: 0.012165580851467983, 13: 0.012988060384730812, 14: 0.012963733748069578, 15: 0.012564062428221393, 16: 0.012544819896366913, 17: 0.01296828672853895, 18: 0.013156264951205017, 19: 0.012368259434404817, 20: 0.012971299215159384, 21: 0.013562386671019451, 22: 0.012357109991672402, 23: 0.013365583938759935, 24: 0.012757474348559256, 25: 0.012965969029621292, 26: 0.012970905007161996, 27: 0.012367189413632747, 28: 0.012769227835381484, 29: 0.013566869773826387, 30: 0.013773607979725315, 31: 0.012966147220171904, 32: 0.012967722590243435, 33: 0.01234077415229502, 34: 0.013562386671019451, 35: 0.013366715656979524, 36: 0.012954799179360214, 37: 0.013367486259910879, 38: 0.01277272725798291, 39: 0.013163901859045268, 40: 0.013363130481388, 41: 0.012973409140277257, 42: 0.02771313443168768, 43: 0.027325795873132074, 44: 0.02691331986602965, 45: 0.02651197764017356, 46: 0.026157179107234613, 47: 0.02592549507036849, 48: 0.02539717371033782, 49: 0.02509151740317956, 50: 0.024754153266826034, 51: 0.02428794839113306, 52: 0.02400545981849922, 53: 0.023631120826291655, 54: 0.022984479431226505, 55: 0.02282122144555069, 56: 0.02254945625324973, 57: 0.022162182164829868, 58: 0.02170704171698066, 59: 0.02164975226307984}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x0xnf-ZctvtC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}